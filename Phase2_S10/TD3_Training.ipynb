{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TD3_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4trAgbjw6dQ",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardayal/EVA/blob/master/Phase2_S10/TD3_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "# Twin Delayed DDPG (TD3) Implementation\n",
        "\n",
        "On a custom car env\n",
        "\n",
        "episodes = 2000\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_yrrZ1mw6dR",
        "colab_type": "code",
        "outputId": "93e0d0e7-8184-4dcb-fcb8-a4c3e0b95362",
        "colab": {}
      },
      "source": [
        "#env render is made using pygame\n",
        "!pip install pygame"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 270kB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-1.9.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_MmCF8aw6dW",
        "colab_type": "code",
        "outputId": "3d5fe395-03c7-4d28-fbc5-1106c672ca61",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsdpK-Yfw6dY",
        "colab_type": "code",
        "outputId": "a7850f5a-3456-4c81-f8a0-b37bb844ea08",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 19 17:22:46 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2GlOSjnw6db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding the env file to path\n",
        "import os\n",
        "import sys\n",
        "env_dir ='/content/drive/My Drive/EVA/Phase2_S10/swiggyfood_v4'\n",
        "sys.path.append(env_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlPUSfe1w6dd",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNMk14jpw6de",
        "colab_type": "code",
        "outputId": "d2b8aaaf-36a8-4da5-8bee-571521964888",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "import math\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "import pygame\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31RLIcTqw6di",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiQ-HEVRw6dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states1, batch_states2, batch_next_states1, batch_next_states2, batch_actions, batch_rewards, batch_dones = [], [], [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state1, state2, next_state1, next_state2, action, reward, done = self.storage[i]\n",
        "      batch_states1.append(state1)\n",
        "      batch_states2.append(np.array(state2, copy=False))\n",
        "      batch_next_states1.append(next_state1)\n",
        "      batch_next_states2.append(np.array(next_state2, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states1), np.array(batch_states2), np.array(batch_next_states1), np.array(batch_next_states2), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTbpPYCSw6dl",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmgqfDl0w6dm",
        "colab_type": "code",
        "outputId": "c691017d-a087-4c05-9da0-c846f0c5f411",
        "colab": {}
      },
      "source": [
        "def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "# conv2d_size_out(conv2d_size_out(60))\n",
        "# conv2d_size_out(60, kernel_size = 5, stride =2)# : 28\n",
        "a = conv2d_size_out((conv2d_size_out(60, kernel_size = 3, stride =2)), kernel_size = 3, stride =1) # 27\n",
        "conv2d_size_out(a, kernel_size = 3, stride =1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj3ZpgCQw6do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AC_conv(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim=1):\n",
        "    super(AC_conv, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(state_dim, 16, kernel_size=3, stride=2)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1)\n",
        "    self.bn2 = nn.BatchNorm2d(16)\n",
        "    self.conv3 = nn.Conv2d(16, 16, kernel_size=3, stride=1)\n",
        "    self.bn3 = nn.BatchNorm2d(16) # sq of an odd number, because just!\n",
        "    self.conv4 = nn.Conv2d(16, 1, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.conv4(x))\n",
        "    # print(x.shape)\n",
        "    return torch.nn.functional.avg_pool2d(x, kernel_size=5, stride = 5)\n",
        "    # return F.relu(self.conv3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jaUHMk-w6ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#New\n",
        "class Actor(AC_conv):\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    AC_conv.__init__(self)\n",
        "    super(Actor, self).__init__()\n",
        "    \n",
        "    linear_input_size = 25+3\n",
        "\n",
        "    self.layer_1 = nn.Linear(linear_input_size, 30)# if on road or sand\n",
        "    self.layer_2 = nn.Linear(30, 50)\n",
        "    self.layer_3 = nn.Linear(50, action_dim)\n",
        "\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = AC_conv.forward(self, x1)\n",
        "    \n",
        "    x = torch.cat(((x1.view(x1.size(0), -1)),\n",
        "                   x2),1)\n",
        "\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    return self.max_action * torch.tanh(self.layer_3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyvjeI3kw6du",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBtJn2Jzw6du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new\n",
        "class Critic(AC_conv):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    AC_conv.__init__(self)\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "\n",
        "    linear_input_size = 25+3  # add state[\"orientation\"]\n",
        "\n",
        "    self.layer_1 = nn.Linear(linear_input_size + action_dim, 30)# if on road or sand\n",
        "    self.layer_2 = nn.Linear(30, 50)\n",
        "    self.layer_3 = nn.Linear(50, 1)\n",
        "\n",
        "    # Defining the second Critic neural network\n",
        "\n",
        "    self.layer_4 = nn.Linear(linear_input_size + action_dim, 30)# if on road or sand\n",
        "    self.layer_5 = nn.Linear(30, 50)\n",
        "    self.layer_6 = nn.Linear(50, 1)\n",
        "\n",
        "\n",
        "  def forward(self, x1, x2, u):\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1_1 = AC_conv.forward(self,x1)\n",
        "    \n",
        "    xu_1 = torch.cat(((x1_1.view(x1_1.size(0), -1)),\n",
        "                   x2, u),1)\n",
        "\n",
        "    x_1 = F.relu(self.layer_1(xu_1))\n",
        "    x_1 = F.relu(self.layer_2(x_1))\n",
        "    x_1 = self.layer_3(x_1)\n",
        "\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    # x1_2 = self.mp2(x1)\n",
        "\n",
        "    x1_2 = AC_conv.forward(self,x1)\n",
        "\n",
        "    xu_2 = torch.cat(((x1_2.view(x1_1.size(0), -1)),\n",
        "                   x2, u),1)\n",
        "    \n",
        "    x_2 = F.relu(self.layer_4(xu_2))\n",
        "    x_2 = F.relu(self.layer_5(x_2))\n",
        "    x_2 = self.layer_6(x_2)\n",
        "    return x_1, x_2\n",
        "\n",
        "  def Q1(self, x1, x2, u):\n",
        "    x1_1 = AC_conv.forward(self,x1)\n",
        "    \n",
        "    xu_1 = torch.cat(((x1_1.view(x1_1.size(0), -1)),\n",
        "                   x2, u),1)\n",
        "\n",
        "    x_1 = F.relu(self.layer_1(xu_1))\n",
        "    x_1 = F.relu(self.layer_2(x_1))\n",
        "    x_1 = self.layer_3(x_1)\n",
        "    return x_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9c8R5RYw6dy",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRUUFsfQw6dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state1, state2):\n",
        "    state1 = torch.from_numpy(state1).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "    state2 = torch.Tensor(state2).unsqueeze(0).to(device)\n",
        "    # print(f'shape of state1: {state1.shape}; state2{state2.shape}')\n",
        "    return self.actor(state1, state2).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      # batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      batch_states1, batch_states2, batch_next_states1, batch_next_states2, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state1 = torch.from_numpy(batch_states1).float().permute(0, 3, 1, 2).to(device)    \n",
        "      state2 = torch.Tensor(batch_states2).to(device)\n",
        "      # next_state1 = torch.Tensor(batch_next_states1).to(device)\n",
        "      next_state1 = torch.from_numpy(batch_next_states1).float().permute(0, 3, 1, 2).to(device)\n",
        "      next_state2 = torch.Tensor(batch_next_states2).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state1, next_state2)\n",
        "            \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state1, next_state2, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state1, state2, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state1, state2, self.actor(state1, state2)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSKcRbGXw6d0",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz3ScoVIw6d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    # print(f'pickup{env.x1, env.y1}; drop{env.x2,env.y2}')\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(obs['surround'],obs['orientation'])\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQkyXVIw6d3",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_r5f80Yw6d3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"SwiggyFood-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "# seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21lE739Mw6d6",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il27pyJtw6d6",
        "colab_type": "code",
        "outputId": "be5fed8a-e9b4-49dc-eb72-cd785b8ab6ee",
        "colab": {}
      },
      "source": [
        "file_name = \"%s_%s\" % (\"TD3\", env_name)\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_SwiggyFood-v0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEJ7-_yvw6d8",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-qC_ZQ0w6d9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = os.path.join(env_dir,\"pytorch_models\")\n",
        "\n",
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(model_path):\n",
        "  os.makedirs(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgHtc0Cfw6eA",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H23Hfd-Jw6eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym_swiggyfood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeG8ZK-Mw6eC",
        "colab_type": "code",
        "outputId": "60d179b2-2885-4900-dd70-604f0652b007",
        "colab": {}
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3i4U5r9w6eE",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment\n",
        "\n",
        "## maybe don't seed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMghbdjGw6eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.manual_seed(seed)\n",
        "# np.random.seed(seed)\n",
        "state_dim = env.observation_space[\"surround\"].shape[2]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "min_action = float(env.action_space.low[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ7uh7ZOw6eH",
        "colab_type": "code",
        "outputId": "70a0de08-970a-46a4-9c34-590028a8f454",
        "colab": {}
      },
      "source": [
        "action_dim, max_action, min_action"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10.0, -10.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP4nWXaqw6eJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_dim1 = env.observation_space[\"surround\"].shape\n",
        "state_dim2 = env.observation_space[\"orientation\"].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzv-o7Yyw6eL",
        "colab_type": "code",
        "outputId": "b1a20b8d-0dd9-4434-b2b3-34df238007fe",
        "colab": {}
      },
      "source": [
        "state_dim, state_dim1, state_dim2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, (60, 60, 1), ())"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1waJIpOHw6eN",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhsA_60Lw6eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBnvuxCPw6eQ",
        "colab_type": "text"
      },
      "source": [
        "## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYNl9l-Lw6eR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxCn4X29w6eT",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NJBUNGfw6eU",
        "colab_type": "code",
        "outputId": "aa617ddd-c4ac-47bd-bc1e-a5ca09eb90aa",
        "colab": {}
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1293.717000\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db_ZrjVVw6eW",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzDRgvZlw6eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVLpilQIw6eY",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PntDgekUw6eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbC3fdadw6eb",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2itZFNIqw6eb",
        "colab_type": "code",
        "outputId": "e6ac6b33-04ce-4783-ed13-99c89060dc7a",
        "colab": {}
      },
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "abc = 0\n",
        "while total_timesteps < max_timesteps:\n",
        "  abc +=1\n",
        "  # print(f'timestamp: {abc}')\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {} on road: {} off road: {}\".format(total_timesteps, episode_num, episode_reward, on_road, off_road))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=model_path)\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "\n",
        "    #pos and neg reward counter\n",
        "    on_road = 0\n",
        "    off_road = 0\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    # action = policy.select_action(np.array(obs))\n",
        "    action = policy.select_action(obs['surround'], obs['orientation'])\n",
        "    \n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # see pos and neg reward counts\n",
        "  if reward >= 0.0:\n",
        "    on_road += 1\n",
        "  else:\n",
        "    off_road += 1  \n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs['surround'], obs['orientation'], new_obs['surround'], new_obs['orientation'], action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=model_path)\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 2000 Episode Num: 1 Reward: -533.6400000000069 on road: 920 off road: 1080\n",
            "Total Timesteps: 4000 Episode Num: 2 Reward: -1401.1300000000015 on road: 304 off road: 1696\n",
            "Total Timesteps: 6000 Episode Num: 3 Reward: -1370.549999999975 on road: 348 off road: 1652\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1636.602000\n",
            "---------------------------------------\n",
            "Total Timesteps: 8000 Episode Num: 4 Reward: -1503.239999999977 on road: 257 off road: 1743\n",
            "Total Timesteps: 10000 Episode Num: 5 Reward: -1635.9399999999869 on road: 202 off road: 1798\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1524.636000\n",
            "---------------------------------------\n",
            "Total Timesteps: 12000 Episode Num: 6 Reward: -1543.849999999984 on road: 229 off road: 1771\n",
            "Total Timesteps: 14000 Episode Num: 7 Reward: -1351.2699999999936 on road: 357 off road: 1643\n",
            "Total Timesteps: 16000 Episode Num: 8 Reward: -436.120000000008 on road: 1072 off road: 928\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1444.631000\n",
            "---------------------------------------\n",
            "Total Timesteps: 18000 Episode Num: 9 Reward: -1721.0199999999752 on road: 103 off road: 1897\n",
            "Total Timesteps: 20000 Episode Num: 10 Reward: -1460.989999999987 on road: 274 off road: 1726\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1700.462000\n",
            "---------------------------------------\n",
            "Total Timesteps: 22000 Episode Num: 11 Reward: -1674.0699999999779 on road: 143 off road: 1857\n",
            "Total Timesteps: 24000 Episode Num: 12 Reward: -1621.6399999999799 on road: 168 off road: 1832\n",
            "Total Timesteps: 26000 Episode Num: 13 Reward: -1642.079999999979 on road: 139 off road: 1861\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1601.205000\n",
            "---------------------------------------\n",
            "Total Timesteps: 28000 Episode Num: 14 Reward: -1820.74999999997 on road: 25 off road: 1975\n",
            "Total Timesteps: 30000 Episode Num: 15 Reward: -1661.439999999977 on road: 154 off road: 1846\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1590.378000\n",
            "---------------------------------------\n",
            "Total Timesteps: 32000 Episode Num: 16 Reward: -1806.8499999999706 on road: 35 off road: 1965\n",
            "Total Timesteps: 34000 Episode Num: 17 Reward: -1820.2999999999702 on road: 25 off road: 1975\n",
            "Total Timesteps: 36000 Episode Num: 18 Reward: -1634.1899999999796 on road: 175 off road: 1825\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1523.421000\n",
            "---------------------------------------\n",
            "Total Timesteps: 38000 Episode Num: 19 Reward: -1672.2399999999773 on road: 135 off road: 1865\n",
            "Total Timesteps: 40000 Episode Num: 20 Reward: -1684.7499999999764 on road: 128 off road: 1872\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1552.221000\n",
            "---------------------------------------\n",
            "Total Timesteps: 42000 Episode Num: 21 Reward: -1658.6899999999782 on road: 135 off road: 1865\n",
            "Total Timesteps: 44000 Episode Num: 22 Reward: -1677.479999999977 on road: 123 off road: 1877\n",
            "Total Timesteps: 46000 Episode Num: 23 Reward: -1807.5499999999709 on road: 36 off road: 1964\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1577.438000\n",
            "---------------------------------------\n",
            "Total Timesteps: 48000 Episode Num: 24 Reward: -1828.0999999999688 on road: 19 off road: 1981\n",
            "Total Timesteps: 50000 Episode Num: 25 Reward: -1658.8699999999776 on road: 155 off road: 1845\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1568.232000\n",
            "---------------------------------------\n",
            "Total Timesteps: 52000 Episode Num: 26 Reward: -1847.1499999999692 on road: 3 off road: 1997\n",
            "Total Timesteps: 54000 Episode Num: 27 Reward: -1829.4099999999694 on road: 16 off road: 1984\n",
            "Total Timesteps: 56000 Episode Num: 28 Reward: -1443.3899999999896 on road: 297 off road: 1703\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1457.381000\n",
            "---------------------------------------\n",
            "Total Timesteps: 58000 Episode Num: 29 Reward: -1245.3199999999986 on road: 447 off road: 1553\n",
            "Total Timesteps: 60000 Episode Num: 30 Reward: -1648.2299999999784 on road: 142 off road: 1858\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1574.539000\n",
            "---------------------------------------\n",
            "Total Timesteps: 62000 Episode Num: 31 Reward: -1066.0300000000034 on road: 593 off road: 1407\n",
            "Total Timesteps: 64000 Episode Num: 32 Reward: -1828.0999999999694 on road: 19 off road: 1981\n",
            "Total Timesteps: 66000 Episode Num: 33 Reward: -1674.7299999999777 on road: 121 off road: 1879\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1461.352000\n",
            "---------------------------------------\n",
            "Total Timesteps: 68000 Episode Num: 34 Reward: -1190.3400000000013 on road: 472 off road: 1528\n",
            "Total Timesteps: 70000 Episode Num: 35 Reward: -1644.6499999999792 on road: 157 off road: 1843\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1301.196000\n",
            "---------------------------------------\n",
            "Total Timesteps: 72000 Episode Num: 36 Reward: -1550.8399999999824 on road: 204 off road: 1796\n",
            "Total Timesteps: 74000 Episode Num: 37 Reward: -1047.5800000000033 on road: 616 off road: 1384\n",
            "Total Timesteps: 76000 Episode Num: 38 Reward: -1533.5399999999845 on road: 217 off road: 1783\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1403.719000\n",
            "---------------------------------------\n",
            "Total Timesteps: 78000 Episode Num: 39 Reward: -1596.9999999999807 on road: 162 off road: 1838\n",
            "Total Timesteps: 80000 Episode Num: 40 Reward: -1825.5499999999693 on road: 21 off road: 1979\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1717.116000\n",
            "---------------------------------------\n",
            "Total Timesteps: 82000 Episode Num: 41 Reward: -1829.4499999999694 on road: 18 off road: 1982\n",
            "Total Timesteps: 84000 Episode Num: 42 Reward: -1813.5899999999704 on road: 28 off road: 1972\n",
            "Total Timesteps: 86000 Episode Num: 43 Reward: -1482.6199999999874 on road: 282 off road: 1718\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1639.923000\n",
            "---------------------------------------\n",
            "Total Timesteps: 88000 Episode Num: 44 Reward: -1753.2499999999736 on road: 84 off road: 1916\n",
            "Total Timesteps: 90000 Episode Num: 45 Reward: -1574.4799999999825 on road: 187 off road: 1813\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1438.263000\n",
            "---------------------------------------\n",
            "Total Timesteps: 92000 Episode Num: 46 Reward: -1818.1599999999705 on road: 28 off road: 1972\n",
            "Total Timesteps: 94000 Episode Num: 47 Reward: -1813.1699999999703 on road: 28 off road: 1972\n",
            "Total Timesteps: 96000 Episode Num: 48 Reward: -1573.1999999999825 on road: 199 off road: 1801\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1577.001000\n",
            "---------------------------------------\n",
            "Total Timesteps: 98000 Episode Num: 49 Reward: -1807.3999999999708 on road: 36 off road: 1964\n",
            "Total Timesteps: 100000 Episode Num: 50 Reward: -1730.929999999974 on road: 89 off road: 1911\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1649.437000\n",
            "---------------------------------------\n",
            "Total Timesteps: 102000 Episode Num: 51 Reward: -1836.3499999999692 on road: 12 off road: 1988\n",
            "Total Timesteps: 104000 Episode Num: 52 Reward: -1821.33999999997 on road: 21 off road: 1979\n",
            "Total Timesteps: 106000 Episode Num: 53 Reward: -1786.9699999999716 on road: 49 off road: 1951\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1678.286000\n",
            "---------------------------------------\n",
            "Total Timesteps: 108000 Episode Num: 54 Reward: -1778.6199999999712 on road: 47 off road: 1953\n",
            "Total Timesteps: 110000 Episode Num: 55 Reward: -1822.2999999999695 on road: 24 off road: 1976\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1696.563000\n",
            "---------------------------------------\n",
            "Total Timesteps: 112000 Episode Num: 56 Reward: -1742.6099999999747 on road: 74 off road: 1926\n",
            "Total Timesteps: 114000 Episode Num: 57 Reward: -1571.4299999999823 on road: 198 off road: 1802\n",
            "Total Timesteps: 116000 Episode Num: 58 Reward: -436.6000000000075 on road: 1070 off road: 930\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1416.795000\n",
            "---------------------------------------\n",
            "Total Timesteps: 118000 Episode Num: 59 Reward: -1616.3999999999792 on road: 162 off road: 1838\n",
            "Total Timesteps: 120000 Episode Num: 60 Reward: -1813.399999999971 on road: 31 off road: 1969\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1506.067000\n",
            "---------------------------------------\n",
            "Total Timesteps: 122000 Episode Num: 61 Reward: -1500.8899999999867 on road: 251 off road: 1749\n",
            "Total Timesteps: 124000 Episode Num: 62 Reward: -1620.8299999999801 on road: 148 off road: 1852\n",
            "Total Timesteps: 126000 Episode Num: 63 Reward: -1726.7099999999743 on road: 86 off road: 1914\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1526.977000\n",
            "---------------------------------------\n",
            "Total Timesteps: 128000 Episode Num: 64 Reward: -69.5399999999984 on road: 1281 off road: 719\n",
            "Total Timesteps: 130000 Episode Num: 65 Reward: -1774.2299999999727 on road: 60 off road: 1940\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1662.993000\n",
            "---------------------------------------\n",
            "Total Timesteps: 132000 Episode Num: 66 Reward: -1595.1999999999816 on road: 183 off road: 1817\n",
            "Total Timesteps: 134000 Episode Num: 67 Reward: -1826.4499999999694 on road: 20 off road: 1980\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}